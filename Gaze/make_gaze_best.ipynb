{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example data loader for Atari-HEAD dataset\n",
    "This file reads dataset by\n",
    "Zhang, R., Walshe, C., Liu, Z., Guan, L., Muller, K., Whritner, J., ... & Ballard, D. (2020, April). Atari-head: Atari human eye-tracking and demonstration dataset. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 6811-6820).\n",
    "'''\n",
    "\n",
    "import sys, os, re, threading, time, copy\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "    width = 84\n",
    "    height = 84\n",
    "    frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    return frame / 255.0\n",
    "\n",
    "class Dataset:\n",
    "  def __init__(self, tar_fname, label_fname):\n",
    "      t1 = time.time()\n",
    "      print(\"Reading all training data into memory...\")\n",
    "\n",
    "      # Read action labels and gaze positions from the txt file\n",
    "      frame_ids, lbls = [], []\n",
    "      gaze_positions = {}\n",
    "\n",
    "      with open(label_fname, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"frame_id\") or line == \"\":\n",
    "                    continue  # skip header or empty lines\n",
    "                dataline = line.split(',')\n",
    "                frame_id = dataline[0]\n",
    "                lbl = dataline[5]\n",
    "                gaze_pos_str = dataline[6:]  # Extract all values after the label\n",
    "\n",
    "                # Convert the list of gaze positions to floats\n",
    "                try:\n",
    "                    gaze_pos = [float(value) for value in gaze_pos_str if value.strip()]\n",
    "                except ValueError:\n",
    "                    gaze_pos = []  # Handle any conversion issues\n",
    "\n",
    "                if lbl == \"null\":  # end of file\n",
    "                    continue\n",
    "\n",
    "                frame_ids.append(frame_id)\n",
    "                lbls.append(int(lbl))\n",
    "\n",
    "                # Initialize the list for this frame_id if it doesn't exist\n",
    "                if frame_id not in gaze_positions:\n",
    "                    gaze_positions[frame_id] = []\n",
    "\n",
    "                # Append the gaze positions to the list for this frame_id\n",
    "                if gaze_pos:\n",
    "                    gaze_positions[frame_id].extend(gaze_pos)\n",
    "                    \n",
    "              \n",
    "     \n",
    "      self.train_lbl = np.asarray(lbls, dtype=np.int32)\n",
    "      self.gaze_positions = gaze_positions\n",
    "      self.train_size = len(self.train_lbl)\n",
    "      self.frame_ids = np.asarray(frame_ids)\n",
    "      print(self.train_size)\n",
    "\n",
    "      # Read training images from tar file\n",
    "      imgs = [None] * self.train_size\n",
    "      print(\"Making a temp dir and uncompressing PNG tar file\")\n",
    "      temp_extract_dir = \"img_data_tmp/\"\n",
    "      if not os.path.exists(temp_extract_dir):\n",
    "          os.mkdir(temp_extract_dir)\n",
    "      tar = tarfile.open(tar_fname, 'r')\n",
    "      tar.extractall(temp_extract_dir)\n",
    "      png_files = tar.getnames()\n",
    "      temp_extract_full_path_dir = temp_extract_dir + png_files[0].split('/')[0]\n",
    "      \n",
    "      \n",
    "      print(\"Uncompressed PNG tar file into temporary directory: \" + temp_extract_full_path_dir)\n",
    "\n",
    "      print(\"Reading images...\")\n",
    "      for i in range(self.train_size):\n",
    "          frame_id = self.frame_ids[i]\n",
    "          png_fname = temp_extract_full_path_dir + '/' + frame_id + '.png'\n",
    "          img = np.float32(cv2.imread(png_fname))\n",
    "          img = preprocess(img)\n",
    "          imgs[i] = copy.deepcopy(img)\n",
    "\n",
    "      self.train_imgs = np.asarray(imgs)\n",
    "      print(\"Time spent to read training data: %.1fs\" % (time.time() - t1))\n",
    "\n",
    "  def standardize(self):\n",
    "      self.mean = np.mean(self.train_imgs, axis=(0, 1, 2))\n",
    "      self.train_imgs -= self.mean  # done in-place --- \"x-=mean\" is faster than \"x=x-mean\"\n",
    "\n",
    "\n",
    "  def load_predicted_gaze_heatmap(self, train_npz):\n",
    "    train_npz = np.load(train_npz)\n",
    "    self.train_GHmap = train_npz['heatmap']\n",
    "    # npz file from pastK models has pastK-fewer data, so we need to know use value of pastK\n",
    "    pastK = 3\n",
    "    self.train_imgs = self.train_imgs[pastK:]\n",
    "    self.train_lbl = self.train_lbl[pastK:]\n",
    "\n",
    "  def reshape_heatmap_for_cgl(self, heatmap_shape):\n",
    "    # predicted human gaze was in 84 x 84, needs to be reshaped for cgl\n",
    "    #heatmap_shape: output feature map size of the conv layer \n",
    "    import cv2\n",
    "    self.temp = np.zeros((len(self.train_GHmap), heatmap_shape, heatmap_shape))\n",
    "    for i in range(len(self.train_GHmap)):\n",
    "        self.temp[i] = cv2.resize(self.train_GHmap[i], (heatmap_shape, heatmap_shape), interpolation=cv2.INTER_AREA)\n",
    "    self.train_GHmap = self.temp\n",
    "\n",
    "  def generate_data_for_gaze_prediction(self):\n",
    "    self.gaze_imgs = []\n",
    "    self.gaze_maps = []\n",
    "    \n",
    "    for i in range(self.train_size):\n",
    "        if i < 3:\n",
    "            # For the first three frames, create a stacked_obs with repeated frames\n",
    "            stacked_obs = np.zeros((84, 84, 4))\n",
    "            for j in range(4):\n",
    "                stacked_obs[:, :, j] = self.train_imgs[max(0, i-j)]\n",
    "        else:\n",
    "            # Regular case for stacking four consecutive frames\n",
    "            stacked_obs = np.zeros((84, 84, 4))\n",
    "            stacked_obs[:, :, 0] = self.train_imgs[i-3]\n",
    "            stacked_obs[:, :, 1] = self.train_imgs[i-2]\n",
    "            stacked_obs[:, :, 2] = self.train_imgs[i-1]\n",
    "            stacked_obs[:, :, 3] = self.train_imgs[i]\n",
    "\n",
    "        self.gaze_imgs.append(copy.deepcopy(stacked_obs))\n",
    "\n",
    "        # Generate gaze map\n",
    "        gaze_map = np.zeros((84, 84, 1))  # Add an extra dimension for the channel\n",
    "        gaze_positions = self.gaze_positions.get(self.frame_ids[i], [])\n",
    "        for x, y in zip(gaze_positions[::2], gaze_positions[1::2]):\n",
    "            x = int(x * 84 / 160)\n",
    "            y = int(y * 84 / 210)\n",
    "            if 0 <= x < 84 and 0 <= y < 84:\n",
    "                cv2.circle(gaze_map[:, :, 0], (x, y), 1, 1, -1)  # Draw on the first channel\n",
    "        self.gaze_maps.append(gaze_map)\n",
    "\n",
    "    self.gaze_imgs = np.asarray(self.gaze_imgs)\n",
    "    self.gaze_maps = np.asarray(self.gaze_maps)\n",
    "    print(\"Shape of the data for gaze prediction: \", self.gaze_imgs.shape)\n",
    "    print(\"Shape of gaze maps: \", self.gaze_maps.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading all training data into memory...\n",
      "17651\n",
      "Making a temp dir and uncompressing PNG tar file\n",
      "Uncompressed PNG tar file into temporary directory: img_data_tmp/52_RZ_2394668_Aug-10-14-52-42\n",
      "Reading images...\n",
      "Time spent to read training data: 16.9s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Model, Sequential \n",
    "\n",
    "def my_softmax(x):\n",
    "    # Reshape the input tensor to flatten the spatial dimensions (84x84)\n",
    "    reshaped_x = tf.reshape(x, (-1, 84 * 84))\n",
    "    \n",
    "    # Apply the softmax along the last dimension\n",
    "    softmaxed_x = tf.nn.softmax(reshaped_x, axis=-1)\n",
    "    \n",
    "    # Reshape it back to the original shape (None, 84, 84, 1)\n",
    "    output = tf.reshape(softmaxed_x, tf.shape(x))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def my_kld(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Correct keras bug. Compute the KL-divergence between two metrics.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-10 # introduce epsilon to avoid log and division by zero error\n",
    "    y_true = K.backend.cast(K.backend.clip(y_true, epsilon, 1), tf.float32)\n",
    "    y_pred = K.backend.cast(K.backend.clip(y_pred, epsilon, 1), tf.float32)\n",
    "    return K.backend.sum(y_true * K.backend.log(y_true / y_pred), axis=[1, 2, 3])\n",
    "\n",
    "\n",
    "\n",
    "def create_saliency_model(input_shape=(84, 84, 4)):\n",
    "    inputs = L.Input(shape=input_shape)\n",
    "    dropout = 0.0\n",
    "    \n",
    "    x=inputs \n",
    "    conv1=L.Conv2D(32, (8,8), strides=4, padding='valid')\n",
    "    x = conv1(x)\n",
    "    x=L.Activation('relu')(x)\n",
    "    x=L.BatchNormalization()(x)\n",
    "    x=L.Dropout(dropout)(x)\n",
    "    \n",
    "    conv2=L.Conv2D(64, (4,4), strides=2, padding='valid')\n",
    "    x = conv2(x)\n",
    "    x=L.Activation('relu')(x)\n",
    "    x=L.BatchNormalization()(x)\n",
    "    x=L.Dropout(dropout)(x)\n",
    "    \n",
    "    conv3=L.Conv2D(64, (3,3), strides=1, padding='valid')\n",
    "    x = conv3(x)\n",
    "    x=L.Activation('relu')(x)\n",
    "    x=L.BatchNormalization()(x)\n",
    "    x=L.Dropout(dropout)(x)\n",
    "    \n",
    "    deconv1 = L.Conv2DTranspose(64, (3,3), strides=1, padding='valid')\n",
    "    x = deconv1(x)\n",
    "    x=L.Activation('relu')(x)\n",
    "    x=L.BatchNormalization()(x)\n",
    "    x=L.Dropout(dropout)(x)\n",
    "\n",
    "    deconv2 = L.Conv2DTranspose(32, (4,4), strides=2, padding='valid')\n",
    "    x = deconv2(x)\n",
    "    x=L.Activation('relu')(x)\n",
    "    x=L.BatchNormalization()(x)\n",
    "    x=L.Dropout(dropout)(x)         \n",
    "\n",
    "    deconv3 = L.Conv2DTranspose(1, (8,8), strides=4, padding='valid')\n",
    "    x = deconv3(x)\n",
    "\n",
    "    outputs = L.Activation(my_softmax)(x)\n",
    "    model=Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    print(\"model created\")\n",
    "    return model\n",
    "\n",
    "\n",
    "tar_file = './ms_pacman/52_RZ_2394668_Aug-10-14-52-42.tar.bz2'\n",
    "label_file = './ms_pacman/52_RZ_2394668_Aug-10-14-52-42.txt'\n",
    "    \n",
    "\n",
    "# Load and preprocess data\n",
    "dataset = Dataset(tar_file, label_file)\n",
    "dataset.generate_data_for_gaze_prediction()\n",
    "\n",
    "\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_saliency_model()\n",
    "# print(model.summary())\n",
    "\n",
    "\n",
    "opt = K.optimizers.Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-08)\n",
    "model.compile(loss=my_kld, optimizer=opt)\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#     predictions = model(dataset.gaze_imgs)\n",
    "#     loss = my_kld(dataset.gaze_maps, predictions)\n",
    "# gradients = tape.gradient(loss, model.trainable_variables)\n",
    "# print(\"Gradients stats: \", [g.numpy().min() for g in gradients], [g.numpy().max() for g in gradients])\n",
    "    \n",
    "\n",
    "# BATCH_SIZE = 50\n",
    "# num_epoch = 50\n",
    "# model.fit(dataset.gaze_imgs, dataset.gaze_maps, BATCH_SIZE, epochs=num_epoch, shuffle=True, verbose=2)\n",
    "# model.save(\"gaze.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_gaze_maps_and_images(dataset):\n",
    "    for i in range(3):  # Save the first 3 gaze maps and corresponding images\n",
    "        # Save the gaze map\n",
    "        gaze_map = dataset.gaze_maps[i].squeeze()  # Remove the extra channel dimension\n",
    "        plt.imshow(gaze_map, cmap='hot', interpolation='nearest')\n",
    "        plt.title(f\"Gaze Map {i + 1}\")\n",
    "        plt.savefig(f\"gaze_map_{i + 1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save the corresponding stacked image\n",
    "        stacked_img = dataset.gaze_imgs[i]\n",
    "        # Since the stacked image has 4 channels, we can visualize each channel separately\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "        for j in range(4):\n",
    "            axs[j].imshow(stacked_img[:, :, j], cmap='gray')\n",
    "            axs[j].set_title(f\"Frame {i + 1}, Channel {j + 1}\")\n",
    "            axs[j].axis('off')\n",
    "        plt.suptitle(f\"Stacked Image {i + 1}\")\n",
    "        plt.savefig(f\"stacked_img_{i + 1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    print(\"Gaze maps and corresponding stacked images saved successfully.\")\n",
    "\n",
    "# Call this function after generating the data\n",
    "save_gaze_maps_and_images(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
